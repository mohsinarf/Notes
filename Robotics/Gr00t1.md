# Robotic Architecture Overview

## 🧠 System 2: Vision-Language Model (VLM) — "The Reasoning Module"
- **Function**: High-level understanding and reasoning based on visual input and language instructions.
- **Hardware**: Runs at **10Hz** on an **NVIDIA L40 GPU** — slower but deliberate processing.
- **Inputs**:
  - Visual perception (e.g., camera feeds)
  - Natural language instructions (e.g., “Pick up the red cup”)
- **Model Type**: Pre-trained **Vision-Language Model** (e.g., CLIP, Flamingo)

---

## 🤖 System 1: Diffusion Transformer for Motion Generation — "The Action Module"
- **Function**: Real-time motor control and action generation.
- **Speed**: Operates at **120Hz** — fast and reactive.
- **Training Method**: Uses **action flow-matching** — learns temporal evolution of actions.
- **Architecture**:
  - **Transformer-based**
  - Uses **cross-attention** to interpret VLM outputs
  - Includes **embodiment-specific encoders/decoders** — adapts to robot hardware (e.g., joints, grippers)
- **Output**: **Closed-loop motor actions** — continuously adjusts using sensor feedback
